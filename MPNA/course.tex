\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}

\usepackage{caption}
%\usepackage{pgfplots}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{footnote}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{url}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{multirow}
\usepackage{amsfonts}
\usepackage[boxed,linesnumbered,noend]{algorithm2e}
\usepackage{qcircuit}
\usepackage{enumitem}
\usepackage{eurosym}
\usepackage{stmaryrd}

\newtheorem{thm}{Theorem}
\newtheorem{prop}{Propriety}
\newtheorem{lemma}{Lemma}
\newtheorem{defi}{Definition}
\newtheorem{coro}{Corollary}



\setlength{\oddsidemargin}{0pt}
% Marge gauche sur pages impaires
\setlength{\evensidemargin}{0pt}
% Marge gauche sur pages paires
\setlength{\textwidth}{470pt}
% Largeur de la zone de texte 
\setlength{\topmargin}{0pt}
% Pas de marge en haut
\setlength{\headheight}{13pt}
% Haut de page
\setlength{\headsep}{10pt}
% Entre le haut de page et le texte
\setlength{\footskip}{40pt}
% Bas de page + séparation
\setlength{\textheight}{630pt}
% Hauteur de la zone de texte

\title{Méthode et Pratiques Scientifiques}
\author{Nahid Ehmad}
\date{}

\newcommand{\note}{\medskip\noindent\underline}
\newcommand{\lin}{\text{lin}\,}

\begin{document}
\maketitle
\tableofcontents
\newpage

Référence : 
\begin{itemize}
\item \textit{Introduction au Calcul Matriciel et Optimisations}, P. Ciarlet 
\end{itemize}

\section{Introduction}
\paragraph{Technique de regroupement}
On cherche à partitionner les sommets $V$ d'un graphe $G=(V,E)$ dans un ensemble de clusters $S_k\subseteq V$ tel que $\bigcup_{k=1}^{p}S_k = V$. La modularité est une mesure de la qualité d'un partitionnement des noeuds dans les communauté. 
\bigskip

La plupart du temps, les solutions à ce problèmes sont NP-Complète : on les approxime par à l'aide d'algorithmes, qui sont principalement basées sur du calcul de valeurs propres. Ce dernier prend 90\% du temps total ! De plus, 90\% du temps passé dans le solveur s'effectue dans des multiplications (sparse) matrice-vecteur .

\subsection{Principaux éléments}
\begin{itemize}
\item Méthode itératives pour problèmes de grande taille
\item Méthodes hybride synchrone/asynchrone
\item Méthode numériques d'algèbre linéaire pour le traitement de masses de données (simulation de phénomènes physiques, analyse des réseaux sociaux, etc)
\item Méthode de compression des structures creuses
\item Modèles de programmation de graphe de Tâches, PGAS
\item Métriques de performances
\end{itemize}

\paragraph{Quelques rappels sur la méthode de Gauss}

\paragraph{Méthode directe vs Itérative} Une méthode directe est une solution dont la solution peut être calculée avec un nombre fini d'opérations arithmétiques élémentaires. A contrario, une méthode itératives est un procédé qui part d'une information initiale arbitraire et renvoie un résultat approché. Pour cela, on réinjecte le résultat en entrée de l'algorithme afin de raffiner la solution.

\paragraph{Exemple} Pour trouver les valeurs propres d'une matrice, il faut trouver les racines du polynôme caractéristique, ce qui est impossible dès que le polynôme dépasse le degrés 4 : on n'utilise donc jamais une résolution directe.
\bigskip

La plupart du temps, les matrices sont creuses : on n'utilise donc pas les mêmes méthodes, plus particulièrement pas de méthodes directes car celle-ci ne sont pas adaptées.
\bigskip

Une des problématiques de nos jours est la consommation électrique. La nouvelle frontière du \emph{calcul exascale} impose des consommations énormes. La machine la plus puissante au 11/16 consommait plus de 15MW !

\subsection{But du cours}
\textit{Le principe de ce cours est la mise au point d'un paradigme de calcul pour la programmation à grande échelle et par la suite en déduire l'introduction de méthodes numériques modernes et "intelligentes" comme celles Krylov hybrides smart-tunées.}


\section{Quel paradigme de programmation ?}
Certains chercheurs pensent directement à OpenMP/MPI lorsque l'on parle d'informatique parallèle. C'est \emph{faux} : il ne s'agit que d'\emph{outils de programmation}, qui ne sont donc pas représentatifs de la théorie derrière.

Par exemple, une multiplication matrice-vecteur puis réduction par addition du vecteur obtenu, il faut faire communiquer ensemble tous les processus : impossible. On rappel que le principal coût énergétique est le déplacement des données !

On cherche donc à :
\begin{itemize}
\item Minimiser le mouvement des données (amoindrir au maximum le coût des communications)
\item Consommation énergétique
\item Parallélisme multi-niveau
\item Ordonnancement multi-niveau
\item Équilibrage de charge multi-niveau
\item Tolérance aux pannes
\end{itemize}


\subsection{Une approche par composants...}
Il faudrait un modèle de programmation laissant la possibilité de définir des blocs autonomes (cf GLCS) appelés \emph{composants} réutilisables dans différents contextes. Par exemple, un composant pour le produit matrice-vecteur creux peut être réutilisé dans un solveur calculant le valeurs propres. Si le travail est bien fait, on peut même réutiliser un composant séquentiel dans un programme parallèle.

\subsection{... eux-même liés par un graph}
On peut définir un problème comme un ensemble de composants liés entre eux par un graph. L'application est donc un ensemble de composants \emph{de très gros grains}, car chaque composant peut être lui-même un graph de composants.

\bigskip
Un projet de recherche à ce sujet est YML (attention, encore expérimental).

\bigskip
Les principales caractéristiques du modèle de programmation recherché sont alors :
\begin{itemize}
\item Privilégier grandement les communication asynchrones sur les communications synchrones
\item Prendre en compte l'hétérogénéité des unités de calcul
\item Prendre en compte la tolérance aux fautes
\item Encourager l'équilibrage de tâche
\item Introduire l'auto-tuning (on choisit automatiquement des paramètres pendant l'exécution avec assistance de l'utilisateur final)
\end{itemize}

\section{Unir et conquérir}


On prend pour exemple la résolution d'un système linéaire $Ax=b$, où $A \in \mathcal{M}_n(\mathbb{C})$ et $b,x\in \mathbb{C}$ sont creux. On cherche quelques \emph{paires de Ritz} $\Lambda_k = (\lambda_1,..,\lambda_k)$ et $U_k = (u_1,...,u_k)$ tel que $\forall i \leq k, Au_i = \lambda_i u_i$.

Ce problème est au cœur de l'algorithme de recherches de données de Google.
\bigskip

\paragraph{Problème :}
Les algorithmes classiques ne sont pas adaptables aux grandes tailles (par exemple, la méthode QR pour trouver les valeurs propres a une précision dépendant de la taille de la matrice). On va donc chercher à réduire notre problème dans un sous-espace de taille moindre, chercher la solution dans ce sous-espace puis effectuer une transformation inverse afin d'obtenir un approximation des solutions recherchés. Le plus souvent, la précision n'est pas suffisante, et on va définir un sous-espace de meilleur qualité pour réitérer le processus.

Pour notre exemple, on va projeter notre matrice dans un sous-espace de Krylov défini par la base $(v, Av, ..., A^{m-1}v)$. On résous le problème de valeur propre dans ce sous-espace ; puis on réitère le processus avec le $v$ trouvé en résolution du système de moins grande taille.

Attention, il ne s'agit pas vraiment d'un méthode itérative, mais plus d'un méthode "à redémarrage" (projection-résolution-retour à l'espace de départ).
\bigskip

La méthode peut être redémarrée explicitement (ERAM/Saad), c'est-à-dire en mettant à jour explicitement $v$ puis recommençant le problème. Par contre, cette méthode n'est pas toujours stable.
Un redémarrage implicite peut aussi être effectué (IRAM/Sorrensen) ; mais repose sur des bases mathématiques plus conséquentes.

Ces méthodes on par contre un inconvénient : si les valeurs propres sont trop approchées les unes des autres, l'algo peut les "confondre".
\bigskip


La méthode \emph{Unite and conquer} consiste à utiliser plusieurs méthodes.


\section{Multiple Krylov Subspace}
On va non pas combiner un ensemble de méthode, mais combiner différentes instances de la même méthode. Par exemple, on va projeter-résoudre-retourner, puis on partage les résultats intermédiaires, et on choisit les meilleurs vecteurs retournés par les instantes. On peut également choisir des tailles de sous-espaces différentes de manière à recouvrer les temps des communications (asynchrones).

La sélection d'une bonne valeur pour la dimension du sous-espace peut être faite par auto-tuning.


\section{Résolution de problèmes de grande taille}
Soit $A\in \mathbb{C}^n$ une matrice creuse. Le problème est de calculer un petit nombre $s$ de valeur propres $\lambda_i$ et ses vecteurs propres (normalisés) correspondants $v_i$ de $A$ : $Av_i = \lambda_i v_i$ pour $i\in \llbracket 1,s \rrbracket$. On suppose les valeurs propres triées par ordre décroissant de module. On note $\lin (s)$ pour désigner tous les vecteurs colinéaires à $s$.

\begin{defi}[Suite de Krylov]
Soit $q$ un vecteur non nul. La suite $(A^nq)^{n \in \mathbb{N}}$ est appelée suite de Krylov.
\end{defi}

\begin{defi}[Sous-espace de Krylov]
Le sous espace de Krylov de taille $m$ est le sous-espace engendré par les $m$ premiers vecteurs de la suite de Krylov si ces vecteurs sont linéairement indépendants.
\end{defi}

\begin{thm}
\[ A^m q \underset{m \to + \infty}{\longrightarrow} lin(v_1)\]
\end{thm}

La méthode de la puissance est aussi appelée méthode des itérations du quotient de Rayleigh. Il s'agit de calculer $A^m q$ pour $m = 1, 2, ...$ jusqu'à convergence.


\paragraph{Méthode des itérations simultanées (IS)}
Soit $S= \begin{pmatrix} q_1 & ... &q_k \end{pmatrix}$ où $q_1,..., q_k$ sont des vecteurs linéairement indépendants. On calcule $S, AS, A^2S, ...$. On peut montrer que le suite $A^mS \underset{m\to +\infty}{\longrightarrow} lin(v_1,v_2,...,v_k)$.

\paragraph{Algorithme d'Arnoldi}
On définit les sous-espaces de Kyrlov $K_m(a,q_i) = lim(q_1, Aq_1,...,A^{m-1}q_1)$. On calcul en fait une base orthonormée.

\begin{algorithm}[H]
$q_1 = \dfrac{q}{||q||}$ \\
\For{k=1 à m-1}{
	$w=Aq_k$\\
	\For{j=1 à k}{
		$h_{j,k}=\langle w,q_0 \rangle$\\
		$w=w-h_{j,k}\cdot q_j$
	}
	$h_{k+1, k}=||w||_2$\\
	$q_{k+1}=\dfrac{w}{h_{k+1,k}}$
}
\end{algorithm}

L'algorithme fournit alors $H_{m,m_1}$ et $Q_m$ tels que $AQ_{m-1}=Q_{m}\cdot H_{m,m-1}$, que l'ont peut réécrire en $AQ_m = Q_{m-1}\cdot H_{m-1,m-1}+q_m\cdot h_{m,m-1} \cdot e_{m-1}^T$ où le second terme est le \emph{résidu} (et $e_{m-1}$ le $m-1$ème vecteur de la base canonique).

\paragraph{Remarques}
\begin{itemize}
\item Si $h_{m,m} = 0 \Rightarrow Q_{m-1}^TAQ_{m-1}=H_{m-1,m-1}$ et les valeurs propres de $H_{m-1,m-1}$ sont celles de $A$.

\item Si $m-1=n \Rightarrow Q-n$ est une base orthonormal de $\mathbb{C}^n$ et les valeurs propres de $H_{n,n}$ sont celles de $A$.

\item Soit $H=H_{m-1,m-1}$ et $Hy=\lambda y$ et $w=Q_{m-1}y$ alors l'équation devient
\[Av=\lambda v + h_{m, m-1}q_m e_{m-1}^T y\]
Soit
\[||Av-\lambda v|| = |h_{m,m-1}|.|y_{m-1}|.\underbrace{||q_m||_2}_{=0}\]
Le résidu est alors $||Av-\lambda v||_2 = |h_{m,m-1}|.|y_{m-1}|$
\end{itemize}


\end{document}